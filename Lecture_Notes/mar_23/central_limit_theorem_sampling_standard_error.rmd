---
title: "Central Limit Theorem, Samping, Standard Error"
author: "Simon J. Kiss"
date: "19/03/2020"
output: slidy_presentation
---

```{r setup, include=FALSE, purl=F}
knitr::opts_chunk$set(cache=T, echo=T, message=F, warning=F)

```

# Learning Outcomes
1. Students will understand to be able to provide a working definition of
    * sample
    * population
    * probability based sample
    * central limit theorem
    * standard error
    * confidence interval
    * margin of error


# The problem
- Difference between a sample and a population
     * Population is the universe of things you want to describe
     * Sample is the subset of the universe you have at hand 
- Different notations are used 

Measure |Sample |Population |
|-----|----|----|
| Average | $$\overline{x}$$|$$ \mu$$ |
| Standard Deviation | $$s $$ | $$ \sigma $$ | 

# The Problem
- We want to know the population average, but we only get to measure a sample average?

Q: How well does the sample average reflect the population average?

A: Remarkably well when a sample is random (probabilistic) and large enough

# The Problem
- Probabilistic (random) sample
     * Ideal way to sample a population; all units of the population must have an equal chance to be selected into the sample
- non-probabilistic sample
     * Less-than-ideal, but still useful
     * Online panels of volunteers (public opinion research companies)
     * Snowball sampling (for hard to reach populations )
     
# The Central Limit Theorem
> The distribution of sample means, *drawn randomly*,  approximates a normal distributions, regardless of the distribution of the underlying population, as the sample gets larger.

* With a sufficiently large sample size, the average of a sample will reliably approach the average of the population

# The Data
- Statistics Canada publishes Public Use Microdata File
- Available through the Laurier Libray, via  [ODESI](https://search2-odesi-ca.libproxy.wlu.ca/#/details?uri=%2Fodesi%2Fpumf-98M0001-E-2016-individuals.xml)
- It contains a *large* probabilistic (random) sample of the 2016 census 
- individual level data on almost 1000000 Canadians 

```{r read-in-data, cache=T}
load(url("https://github.com/sjkiss/DMJN328/raw/master/Lecture_Notes/mar_23/data/census_2016.rdata"))
```

# Example
- First we use the `look_for()` command in the `labelled` library

```{r look-for-wages}
library(labelled)
look_for(census_2016, "wage")

```
- So now we have the `Wages` variable

# Example
- Check the average
```{r check-mean}
mean(census_2016$Wages, na.rm=T)
```
- So this could be considered the average salary for all of Canadians, i.e. the *population* average

# Visualize Population Wages
```{r graph-histogram}
library(tidyverse)
census_2016 %>% 
  ggplot(., aes(x=Wages))+geom_histogram(bins=200)

```

# Sampling from a Population
- Select only the wages variable
```{r select-only-wages}
census_2016 %>% 
  select(Wages)-> df
```

# Sampling from a Population
- Take 100 random samples of size 5 

```{r sample-size-5, echo=T, results="markup"}
#100 times
100 %>% 
  #rerun the next command; sample_n takes random samples
  #na.omit deletes missing values, 5 is the size of the sample
  rerun(sample_n(na.omit(df), 5)) %>% 
  #calculate the average of each sample 
  map_df(~summarize(., avg=mean(Wages))) ->n5
#print the results 

```

# Sampling from a Population
## Automated Example
- Take 100 random samples of size 5 

```{r print-sample-size-5,echo=T, results="markup"}
print(n5)

```

# Sampling from a Population
## Automated Example
- Take 100 random samples of size 10

```{r sample-size-10, echo=T, results="markup"}
#100 times
100 %>%
  #rerun the next command; sample_n takes random samples
  #na.omit deletes missing values, 5 is the size of the sample
  rerun(sample_n(na.omit(df), 10)) %>%
  #calculate the average of each sample
  map_df(~summarize(., avg=mean(Wages))) ->n10

```

# Sampling from a Population
## Automated Example
- Take 100 random samples of size 10
```{r print-n-10}
print(n10)
```


# Sampling from a Population

- Take 100 random samples of size 100

```{r sample-size-100, echo=T, results="markup"}
#100 times
100 %>%
  #rerun the next command; sample_n takes random samples
  #na.omit deletes missing values, 5 is the size of the sample
  rerun(sample_n(na.omit(df), 100)) %>%
  #calculate the average of each sample
  map_df(~summarize(., avg=mean(Wages))) ->n100


```

# Sampling from a Population

- Take 100 random samples of size 100

```{r print-n10, echo=T,  results="markup"}
print(n100)
```

# Sampling from a Population

- Take 100 random samples of size 500

```{r sample-size-500, echo=T, results="markup"}
#100 times
100 %>%
  #rerun the next command; sample_n takes random samples
  #na.omit deletes missing values, 5 is the size of the sample
  rerun(sample_n(na.omit(df), 500)) %>%
  #calculate the average of each sample
  map_df(~summarize(., avg=mean(Wages))) ->n500
#print the results

```

# Sampling from a Population


```{r print-n500, echo=T, results="markup"}
print(n500)
```

# Sampling from a Population

- Show the distribution of all the averages from the four different sample sizes
- What do you notice about the samples of different sizes?
```{r, combine-data-frame, echo=F, eval=T,fig.width=6, fig.height=5}
#combine into a data frame with four variables
data.frame(n5=n5$avg, n10=n10$avg, n100=n100$avg, n500=n500$avg) %>%
#gather them down into two variables called sample_size and average
  gather(sample_size, average) %>%
  #Change the order of the levels in the sample_size variable; this is strictly aesthetic 
  mutate(sample_size=fct_relevel(sample_size, "n5", "n10", "n100", "n500")) %>%
  #Show a histogram of all the averages for each sample size
  ggplot(., aes(x=average))+
  geom_histogram()+
  facet_wrap(~sample_size)+theme_bw()


```




# Standard Error
- the width of the distribution of sample means can be measured with a measure of dispersion
- Standard Error

$$ SE = \frac{s}{\sqrt{n}} $$ 
- the larger the Standard Error, the larger the sampling distribution. 

# Relationship between sample size and standard error
- First we take samples of different sizes
- start with 1 sample of size 5
```{r sd5, echo=T}
#Take 1 sample of size 5 from df$Wages
sample_n(na.omit(df), 5) %>% 
  #summrize that sample calculating the averge wage, the standard deviation of the wages and how large the sample his
  summarize(avg=mean(Wages), sd=sd(Wages), n=n())-> sd5
```

# Relationship between sample size and standard error
- First we take samples of different sizes
- start with 1 sample of size 10
```{r sd10, echo=T}
#Take 1 sample of size 10 from df$Wages
sample_n(na.omit(df), 10) %>% 
  #summrize that sample calculating the averge wage, the standard deviation of the wages and how large the sample his
  summarize(avg=mean(Wages), sd=sd(Wages), n=n())-> sd10
```




# Relationship between sample size and standard error
- First we take samples of different sizes
- start with 1 sample of size 100
```{r one-sample-size-100, echo=T}
#Take 1 sample of size 5 from df$Wages
sample_n(na.omit(df), 100) %>% 
  #summrize that sample calculating the averge wage, the standard deviation of the wages and how large the sample his
  summarize(avg=mean(Wages), sd=sd(Wages), n=n())-> sd100
```

# Relationship between sample size and standard error
- First we take samples of different sizes
- start with 1 sample of size 500
```{r one-sample-size-500, echo=T}
#Take 1 sample of size 5 from df$Wages
sample_n(na.omit(df), 500) %>% 
  #summrize that sample calculating the averge wage, the standard deviation of the wages and how large the sample his
  summarize(avg=mean(Wages), sd=sd(Wages), n=n())-> sd500
```

# Relationship between sample size and standard error
- Combine all into one and calculate the standard error. 

$$ SE = \frac{s}{\sqrt{n}}$$
```{r combine-into-data-frame}
#Combine all in one
#the bind_rows() function works when data frames have exactly the same variable names. 
combined<-bind_rows(sd5, sd10, sd100, sd500)
# Calculate the standard error
combined<-mutate(combined, se=sd/sqrt(n))
```


# Relationship between sample size and standard error

```{r graph-standard-error-sample-size, fig.width=5, fig.height=4}
ggplot(combined, aes(x=n, y=se))+
  geom_point(size=2)+
  geom_line()+
  theme_bw()+
  labs(title="The Relationship Between Sample Size\nand Standard Error")
```

# Confidence Intervals
- Because of the CLT, all possible samples are normally distributed
- Because of the Empirical rule (68-95-99), we know that 68\% of samples are within one standard deviation (standard error), and 95\% of samples are within two standard deviations. 
- we can use this to quanitfy the uncertainty associted with any survey measurement. 

# Confidence Intervals
- Take our last sample of 500 
- The average is `r format(sd500$avg,scientific=F)` with a standard error of `r round(combined[4,4],1)`

```{r, echo=T}
print(combined)

```

# Confidence Intervals
Q: What is the 95% confidence interval for our measurement? 

Hint: How many standard deviations above and below a variable's average are 95\% 

# Confidence Intervals
What does the range mean?
- Technically, it means that 95\% of random samples with this size and this standard deviation would produce an average Wage within these values 
- Non-technically, it means that we can be 95\% confident that the Canadian average wage lies between these two values

# Confidence Intervals
## Margin of Error in Polls
from The Hill Times

> Just more than 1 in 3 Americans — 37 percent — said in a new poll that they have a good amount or a great deal of trust in the information they hear about coronavirus from President Trump. ... The survey of 835 adults was conducted on Friday and Saturday. It has an overall margin of error of 4.8 percentage points. Among 784 registered voters, the margin of error is 4.9 percentage points.  

[Source: The Hill Times](https://thehill.com/homenews/administration/487956-poll-37-of-americans-trust-trump-on-coronavirus)


